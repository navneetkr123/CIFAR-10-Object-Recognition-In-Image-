{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSUog0K99Ylf",
        "colab_type": "text"
      },
      "source": [
        "# Object Classification with Convolutional Neural Networks in the Keras Deep Learning Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnHchsyL8K1C",
        "colab_type": "text"
      },
      "source": [
        "# Reference:\n",
        "1.https://keras.io/examples/cifar10_cnn/\n",
        "\n",
        "**Since I had to train model on Cifar10 datasets so for this I am loading the dataset from keras because it's already there.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OW8CPV08n8Y",
        "colab_type": "text"
      },
      "source": [
        "# Here I have implemented two models one with 7 convolution layer and other with 5 convolution layer.\n",
        "1. Accuracy with 7 convolution layer =81.32%\n",
        "2. Accuracy with 5 convolution layer = 80.72%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSm4_gpw-AcR",
        "colab_type": "text"
      },
      "source": [
        "# Importing all the necessary library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQibeUbs3AUU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2327a6aa-b495-40bc-853e-b089a8107b2b"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.initializers import he_normal, glorot_normal\n",
        "\n",
        "# import BatchNormalization\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIZb96QM3KBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9245bf7f-039a-47d2-9353-bc68c86c3326"
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 128\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbn_Wuu--Ki_",
        "colab_type": "text"
      },
      "source": [
        "**Normalizing the train and test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BZH_IOLVIi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am-Y7MLi3KOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nQE-wKE-Z8p",
        "colab_type": "text"
      },
      "source": [
        "**Function for cross_entropy vs epochs plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivg97lW43KTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "#%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "# https://gist.github.com/greydanus/f6eee59eaf1d90fcb3b534a25362cea4\n",
        "# https://stackoverflow.com/a/14434334\n",
        "# this function is used to update the plots for each epoch and error\n",
        "def plt_dynamic(x, vy, ty, ax, colors=['b']):\n",
        "    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n",
        "    ax.plot(x, ty, 'r', label=\"Train Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    fig.canvas.draw()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf2cpY82-nWx",
        "colab_type": "text"
      },
      "source": [
        "# Convolution layer 7*7 + Dropout +max pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIslcw73kko",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1159
        },
        "outputId": "919ee241-6bb5-4745-9c28-a488644f21c8"
      },
      "source": [
        "# Initialising the model\n",
        "model_7 = Sequential()\n",
        "\n",
        "# Adding first conv layer\n",
        "model_7.add(Conv2D(32, kernel_size=(2, 2),padding='same',activation='relu',input_shape=x_train.shape[1:]))\n",
        "\n",
        "# Adding second conv layer\n",
        "model_7.add(Conv2D(32, (2, 2), activation='relu'))\n",
        "\n",
        "# Adding Maxpooling layer\n",
        "model_7.add(MaxPooling2D(pool_size=(3, 3), strides=(1,1)))\n",
        "\n",
        "# Adding Dropout\n",
        "model_7.add(Dropout(0.3))\n",
        "\n",
        "# Adding third conv layer\n",
        "model_7.add(Conv2D(64, (2, 2), activation='relu'))\n",
        "\n",
        "# Adding Maxpooling layer\n",
        "model_7.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "\n",
        "\n",
        "# Adding fourth conv layer\n",
        "model_7.add(Conv2D(64, (2, 2),padding='same',activation='relu'))\n",
        "\n",
        "# Adding fifth conv layer\n",
        "model_7.add(Conv2D(128, (2, 2), activation='relu'))\n",
        "\n",
        "# Adding Maxpooling layer\n",
        "model_7.add(MaxPooling2D(pool_size=(3, 3),padding='same'))\n",
        "\n",
        "# Adding Dropout\n",
        "model_7.add(Dropout(0.25))\n",
        "\n",
        "# Adding sixth conv layer\n",
        "model_7.add(Conv2D(128, (2, 2),padding='same',activation='relu'))\n",
        "\n",
        "# Adding seventh conv layer\n",
        "model_7.add(Conv2D(256, (2, 2), activation='relu'))\n",
        "\n",
        "# Adding Maxpooling layer\n",
        "model_7.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1)))\n",
        "\n",
        "# Adding Dropout\n",
        "model_7.add(Dropout(0.25))\n",
        "\n",
        "# Adding flatten layer\n",
        "model_7.add(Flatten())\n",
        "\n",
        "# Adding first hidden layer\n",
        "model_7.add(Dense(256, activation='relu',kernel_initializer=he_normal(seed=None)))\n",
        "\n",
        "# Adding Batch Normalization\n",
        "model_7.add(BatchNormalization())\n",
        "\n",
        "# Adding Dropout\n",
        "model_7.add(Dropout(0.5))\n",
        "\n",
        "# Adding second hidden layer\n",
        "model_7.add(Dense(128, activation='relu',kernel_initializer=he_normal(seed=None)))\n",
        "\n",
        "# Adding Dropout\n",
        "model_7.add(Dropout(0.25))\n",
        "\n",
        "# Adding output layer\n",
        "model_7.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Printing model Summary\n",
        "print(model_7.summary())\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0618 12:34:00.139813 139827042133888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0618 12:34:00.152330 139827042133888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0618 12:34:00.154792 139827042133888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0618 12:34:00.179486 139827042133888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0618 12:34:00.181853 139827042133888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0618 12:34:00.191115 139827042133888 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0618 12:34:00.302188 139827042133888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        416       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 31, 31, 32)        4128      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 29, 29, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 29, 29, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 64)        16448     \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 13, 13, 128)       32896     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 5, 5, 128)         65664     \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 256)         131328    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 884,426\n",
            "Trainable params: 883,914\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD7ZjHSj-4XV",
        "colab_type": "text"
      },
      "source": [
        "**Earlier i made model without augmentation accuracy I got was 72%, but after reading few blogs I added data augmentation and accuracy increased by 10 %**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfCcqXddWDeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data augmentation\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcgv08ik_Q1q",
        "colab_type": "text"
      },
      "source": [
        "# Fitting model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsXmKx2M6KK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4474
        },
        "outputId": "0d4f59f0-baef-47c9-8c2d-8cce4aea1d1a"
      },
      "source": [
        "# Compiling the model\n",
        "model_7.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fitting the data to the model\n",
        "history_7 = model_7.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0618 12:34:00.878642 139827042133888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0618 12:34:01.075519 139827042133888 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "390/390 [==============================] - 26s 65ms/step - loss: 1.8123 - acc: 0.3152 - val_loss: 1.9699 - val_acc: 0.3006\n",
            "Epoch 2/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 1.4615 - acc: 0.4671 - val_loss: 1.4420 - val_acc: 0.4822\n",
            "Epoch 3/128\n",
            "390/390 [==============================] - 27s 69ms/step - loss: 1.3313 - acc: 0.5203 - val_loss: 1.2838 - val_acc: 0.5414\n",
            "Epoch 4/128\n",
            "390/390 [==============================] - 26s 68ms/step - loss: 1.2314 - acc: 0.5554 - val_loss: 1.4992 - val_acc: 0.4965\n",
            "Epoch 5/128\n",
            "390/390 [==============================] - 25s 65ms/step - loss: 1.1659 - acc: 0.5831 - val_loss: 1.3098 - val_acc: 0.5597\n",
            "Epoch 6/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 1.1136 - acc: 0.6023 - val_loss: 1.0875 - val_acc: 0.6160\n",
            "Epoch 7/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 1.0553 - acc: 0.6274 - val_loss: 0.9141 - val_acc: 0.6729\n",
            "Epoch 8/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 1.0125 - acc: 0.6438 - val_loss: 1.0940 - val_acc: 0.6119\n",
            "Epoch 9/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.9944 - acc: 0.6513 - val_loss: 0.9142 - val_acc: 0.6736\n",
            "Epoch 10/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.9597 - acc: 0.6640 - val_loss: 1.3719 - val_acc: 0.5713\n",
            "Epoch 11/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.9292 - acc: 0.6763 - val_loss: 0.8833 - val_acc: 0.6850\n",
            "Epoch 12/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.9012 - acc: 0.6849 - val_loss: 0.9451 - val_acc: 0.6738\n",
            "Epoch 13/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.8798 - acc: 0.6934 - val_loss: 1.0562 - val_acc: 0.6467\n",
            "Epoch 14/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.8573 - acc: 0.7032 - val_loss: 0.9235 - val_acc: 0.6875\n",
            "Epoch 15/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.8423 - acc: 0.7090 - val_loss: 0.8842 - val_acc: 0.6939\n",
            "Epoch 16/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.8315 - acc: 0.7107 - val_loss: 0.9825 - val_acc: 0.6708\n",
            "Epoch 17/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.8232 - acc: 0.7161 - val_loss: 0.9171 - val_acc: 0.6914\n",
            "Epoch 18/128\n",
            "390/390 [==============================] - 23s 58ms/step - loss: 0.8138 - acc: 0.7197 - val_loss: 1.0103 - val_acc: 0.6696\n",
            "Epoch 19/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.7898 - acc: 0.7274 - val_loss: 0.9248 - val_acc: 0.6909\n",
            "Epoch 20/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.7855 - acc: 0.7297 - val_loss: 0.8354 - val_acc: 0.7166\n",
            "Epoch 21/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.7662 - acc: 0.7360 - val_loss: 0.8996 - val_acc: 0.6966\n",
            "Epoch 22/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.7618 - acc: 0.7389 - val_loss: 0.6778 - val_acc: 0.7633\n",
            "Epoch 23/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.7485 - acc: 0.7440 - val_loss: 0.7747 - val_acc: 0.7330\n",
            "Epoch 24/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.7422 - acc: 0.7430 - val_loss: 0.8769 - val_acc: 0.7085\n",
            "Epoch 25/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.7266 - acc: 0.7499 - val_loss: 0.8060 - val_acc: 0.7296\n",
            "Epoch 26/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.7295 - acc: 0.7518 - val_loss: 0.7998 - val_acc: 0.7293\n",
            "Epoch 27/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.7188 - acc: 0.7508 - val_loss: 0.8475 - val_acc: 0.7170\n",
            "Epoch 28/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.7076 - acc: 0.7545 - val_loss: 0.9577 - val_acc: 0.6890\n",
            "Epoch 29/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.7077 - acc: 0.7583 - val_loss: 0.6999 - val_acc: 0.7584\n",
            "Epoch 30/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6931 - acc: 0.7595 - val_loss: 0.8102 - val_acc: 0.7308\n",
            "Epoch 31/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6951 - acc: 0.7615 - val_loss: 0.7812 - val_acc: 0.7376\n",
            "Epoch 32/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6867 - acc: 0.7652 - val_loss: 0.6439 - val_acc: 0.7801\n",
            "Epoch 33/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6814 - acc: 0.7650 - val_loss: 0.8172 - val_acc: 0.7189\n",
            "Epoch 34/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6776 - acc: 0.7701 - val_loss: 0.7939 - val_acc: 0.7277\n",
            "Epoch 35/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.6701 - acc: 0.7711 - val_loss: 0.7312 - val_acc: 0.7518\n",
            "Epoch 36/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6673 - acc: 0.7732 - val_loss: 0.6634 - val_acc: 0.7747\n",
            "Epoch 37/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.6588 - acc: 0.7742 - val_loss: 0.7918 - val_acc: 0.7367\n",
            "Epoch 38/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6586 - acc: 0.7742 - val_loss: 0.8533 - val_acc: 0.7150\n",
            "Epoch 39/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6493 - acc: 0.7769 - val_loss: 0.7293 - val_acc: 0.7531\n",
            "Epoch 40/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6445 - acc: 0.7811 - val_loss: 0.7833 - val_acc: 0.7424\n",
            "Epoch 41/128\n",
            "390/390 [==============================] - 23s 58ms/step - loss: 0.6448 - acc: 0.7795 - val_loss: 0.6587 - val_acc: 0.7748\n",
            "Epoch 42/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6328 - acc: 0.7807 - val_loss: 0.7310 - val_acc: 0.7556\n",
            "Epoch 43/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6305 - acc: 0.7845 - val_loss: 0.6404 - val_acc: 0.7828\n",
            "Epoch 44/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6239 - acc: 0.7876 - val_loss: 0.6743 - val_acc: 0.7741\n",
            "Epoch 45/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6300 - acc: 0.7863 - val_loss: 0.7796 - val_acc: 0.7393\n",
            "Epoch 46/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6247 - acc: 0.7874 - val_loss: 0.6791 - val_acc: 0.7704\n",
            "Epoch 47/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6116 - acc: 0.7924 - val_loss: 0.6694 - val_acc: 0.7774\n",
            "Epoch 48/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6161 - acc: 0.7898 - val_loss: 0.6557 - val_acc: 0.7806\n",
            "Epoch 49/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.6074 - acc: 0.7926 - val_loss: 0.7279 - val_acc: 0.7550\n",
            "Epoch 50/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6092 - acc: 0.7924 - val_loss: 0.7603 - val_acc: 0.7553\n",
            "Epoch 51/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.6027 - acc: 0.7947 - val_loss: 0.6830 - val_acc: 0.7685\n",
            "Epoch 52/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5914 - acc: 0.7987 - val_loss: 0.6177 - val_acc: 0.7863\n",
            "Epoch 53/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.6019 - acc: 0.7945 - val_loss: 0.6772 - val_acc: 0.7709\n",
            "Epoch 54/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.5944 - acc: 0.7963 - val_loss: 0.7794 - val_acc: 0.7432\n",
            "Epoch 55/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5954 - acc: 0.7962 - val_loss: 0.6898 - val_acc: 0.7623\n",
            "Epoch 56/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5824 - acc: 0.8017 - val_loss: 0.5831 - val_acc: 0.8009\n",
            "Epoch 57/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5855 - acc: 0.8024 - val_loss: 0.8326 - val_acc: 0.7376\n",
            "Epoch 58/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5797 - acc: 0.8027 - val_loss: 0.5984 - val_acc: 0.7963\n",
            "Epoch 59/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5823 - acc: 0.8013 - val_loss: 0.6417 - val_acc: 0.7801\n",
            "Epoch 60/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5754 - acc: 0.8032 - val_loss: 0.5733 - val_acc: 0.8025\n",
            "Epoch 61/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5671 - acc: 0.8052 - val_loss: 0.8011 - val_acc: 0.7424\n",
            "Epoch 62/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5776 - acc: 0.8039 - val_loss: 0.6422 - val_acc: 0.7815\n",
            "Epoch 63/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5671 - acc: 0.8083 - val_loss: 0.5886 - val_acc: 0.7970\n",
            "Epoch 64/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5611 - acc: 0.8087 - val_loss: 0.7627 - val_acc: 0.7575\n",
            "Epoch 65/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5614 - acc: 0.8098 - val_loss: 0.6294 - val_acc: 0.7878\n",
            "Epoch 66/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5580 - acc: 0.8102 - val_loss: 0.7460 - val_acc: 0.7544\n",
            "Epoch 67/128\n",
            "390/390 [==============================] - 23s 58ms/step - loss: 0.5614 - acc: 0.8096 - val_loss: 0.6430 - val_acc: 0.7812\n",
            "Epoch 68/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5578 - acc: 0.8113 - val_loss: 0.8380 - val_acc: 0.7386\n",
            "Epoch 69/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5512 - acc: 0.8125 - val_loss: 0.6608 - val_acc: 0.7760\n",
            "Epoch 70/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5491 - acc: 0.8129 - val_loss: 0.6581 - val_acc: 0.7817\n",
            "Epoch 71/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5478 - acc: 0.8137 - val_loss: 0.5561 - val_acc: 0.8061\n",
            "Epoch 72/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5392 - acc: 0.8165 - val_loss: 0.5504 - val_acc: 0.8127\n",
            "Epoch 73/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5451 - acc: 0.8154 - val_loss: 0.7322 - val_acc: 0.7584\n",
            "Epoch 74/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5421 - acc: 0.8165 - val_loss: 0.7626 - val_acc: 0.7552\n",
            "Epoch 75/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5400 - acc: 0.8156 - val_loss: 0.7371 - val_acc: 0.7559\n",
            "Epoch 76/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.5330 - acc: 0.8192 - val_loss: 0.5555 - val_acc: 0.8113\n",
            "Epoch 77/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5389 - acc: 0.8164 - val_loss: 0.6962 - val_acc: 0.7706\n",
            "Epoch 78/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5354 - acc: 0.8186 - val_loss: 0.5969 - val_acc: 0.8029\n",
            "Epoch 79/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5296 - acc: 0.8192 - val_loss: 0.6385 - val_acc: 0.7828\n",
            "Epoch 80/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5315 - acc: 0.8192 - val_loss: 0.5574 - val_acc: 0.8078\n",
            "Epoch 81/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5270 - acc: 0.8193 - val_loss: 0.7093 - val_acc: 0.7701\n",
            "Epoch 82/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5229 - acc: 0.8226 - val_loss: 0.6799 - val_acc: 0.7750\n",
            "Epoch 83/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5223 - acc: 0.8221 - val_loss: 0.5801 - val_acc: 0.8000\n",
            "Epoch 84/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5230 - acc: 0.8231 - val_loss: 0.6376 - val_acc: 0.7933\n",
            "Epoch 85/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5133 - acc: 0.8261 - val_loss: 0.5462 - val_acc: 0.8110\n",
            "Epoch 86/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.5178 - acc: 0.8223 - val_loss: 0.5505 - val_acc: 0.8121\n",
            "Epoch 87/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5197 - acc: 0.8205 - val_loss: 0.5906 - val_acc: 0.8064\n",
            "Epoch 88/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5179 - acc: 0.8249 - val_loss: 0.5773 - val_acc: 0.8032\n",
            "Epoch 89/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5075 - acc: 0.8264 - val_loss: 0.6454 - val_acc: 0.7896\n",
            "Epoch 90/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5129 - acc: 0.8263 - val_loss: 0.7147 - val_acc: 0.7704\n",
            "Epoch 91/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5031 - acc: 0.8287 - val_loss: 0.5914 - val_acc: 0.8002\n",
            "Epoch 92/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5135 - acc: 0.8250 - val_loss: 0.6457 - val_acc: 0.7867\n",
            "Epoch 93/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5027 - acc: 0.8283 - val_loss: 0.5913 - val_acc: 0.8018\n",
            "Epoch 94/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4987 - acc: 0.8301 - val_loss: 0.5026 - val_acc: 0.8263\n",
            "Epoch 95/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5001 - acc: 0.8303 - val_loss: 0.5791 - val_acc: 0.8040\n",
            "Epoch 96/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.5014 - acc: 0.8286 - val_loss: 0.6168 - val_acc: 0.7958\n",
            "Epoch 97/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4981 - acc: 0.8284 - val_loss: 0.5013 - val_acc: 0.8255\n",
            "Epoch 98/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4929 - acc: 0.8302 - val_loss: 0.6024 - val_acc: 0.7995\n",
            "Epoch 99/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4906 - acc: 0.8342 - val_loss: 0.4929 - val_acc: 0.8301\n",
            "Epoch 100/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.4909 - acc: 0.8340 - val_loss: 0.5325 - val_acc: 0.8201\n",
            "Epoch 101/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4997 - acc: 0.8293 - val_loss: 0.5976 - val_acc: 0.7989\n",
            "Epoch 102/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4942 - acc: 0.8316 - val_loss: 0.5568 - val_acc: 0.8134\n",
            "Epoch 103/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4856 - acc: 0.8334 - val_loss: 0.7222 - val_acc: 0.7613\n",
            "Epoch 104/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4900 - acc: 0.8325 - val_loss: 0.5549 - val_acc: 0.8142\n",
            "Epoch 105/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4910 - acc: 0.8325 - val_loss: 0.5394 - val_acc: 0.8199\n",
            "Epoch 106/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4820 - acc: 0.8344 - val_loss: 0.5473 - val_acc: 0.8118\n",
            "Epoch 107/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4828 - acc: 0.8353 - val_loss: 0.6925 - val_acc: 0.7808\n",
            "Epoch 108/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4906 - acc: 0.8353 - val_loss: 0.5173 - val_acc: 0.8260\n",
            "Epoch 109/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4818 - acc: 0.8356 - val_loss: 0.5582 - val_acc: 0.8121\n",
            "Epoch 110/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4820 - acc: 0.8341 - val_loss: 0.5043 - val_acc: 0.8251\n",
            "Epoch 111/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4812 - acc: 0.8358 - val_loss: 0.5702 - val_acc: 0.8087\n",
            "Epoch 112/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4783 - acc: 0.8368 - val_loss: 0.5598 - val_acc: 0.8104\n",
            "Epoch 113/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4745 - acc: 0.8383 - val_loss: 0.5448 - val_acc: 0.8193\n",
            "Epoch 114/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4839 - acc: 0.8350 - val_loss: 0.5644 - val_acc: 0.8124\n",
            "Epoch 115/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.4723 - acc: 0.8388 - val_loss: 0.6279 - val_acc: 0.7981\n",
            "Epoch 116/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.4699 - acc: 0.8401 - val_loss: 0.5355 - val_acc: 0.8167\n",
            "Epoch 117/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4767 - acc: 0.8383 - val_loss: 0.6175 - val_acc: 0.7972\n",
            "Epoch 118/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4624 - acc: 0.8434 - val_loss: 0.5375 - val_acc: 0.8178\n",
            "Epoch 119/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.4651 - acc: 0.8396 - val_loss: 0.6269 - val_acc: 0.7920\n",
            "Epoch 120/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.4649 - acc: 0.8426 - val_loss: 0.6274 - val_acc: 0.7975\n",
            "Epoch 121/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.4691 - acc: 0.8397 - val_loss: 0.5931 - val_acc: 0.8071\n",
            "Epoch 122/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4652 - acc: 0.8418 - val_loss: 0.6465 - val_acc: 0.7891\n",
            "Epoch 123/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4649 - acc: 0.8414 - val_loss: 0.5457 - val_acc: 0.8186\n",
            "Epoch 124/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4585 - acc: 0.8434 - val_loss: 0.4930 - val_acc: 0.8336\n",
            "Epoch 125/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4593 - acc: 0.8450 - val_loss: 0.5677 - val_acc: 0.8135\n",
            "Epoch 126/128\n",
            "390/390 [==============================] - 22s 56ms/step - loss: 0.4642 - acc: 0.8410 - val_loss: 0.6462 - val_acc: 0.7862\n",
            "Epoch 127/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4629 - acc: 0.8424 - val_loss: 0.6611 - val_acc: 0.7831\n",
            "Epoch 128/128\n",
            "390/390 [==============================] - 22s 57ms/step - loss: 0.4596 - acc: 0.8429 - val_loss: 0.5598 - val_acc: 0.8132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Drr_jj_aUc",
        "colab_type": "text"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBpjDa8E3kns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "ce0b66e0-818d-4336-c834-b796a59f78cd"
      },
      "source": [
        "# Evaluating the model\n",
        "score = model_7.evaluate(x_test, y_test, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "fig,ax = plt.subplots(1,1)\n",
        "ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
        "\n",
        "# list of epoch numbers\n",
        "x = list(range(1,epochs+1))\n",
        "\n",
        "# print(history.history.keys())\n",
        "# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
        "# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n",
        "\n",
        "# we will get val_loss and val_acc only when you pass the paramter validation_data\n",
        "# val_loss : validation loss\n",
        "# val_acc : validation accuracy\n",
        "\n",
        "# loss : training loss\n",
        "# acc : train accuracy\n",
        "# for each key in histrory.histrory we will have a list of length equal to number of epochs\n",
        "\n",
        "vy = history_7.history['val_loss']\n",
        "ty = history_7.history['loss']\n",
        "plt_dynamic(x, vy, ty, ax)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.5598417278766632\n",
            "Test accuracy: 0.8132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awge9JvaZpgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save to disk\n",
        "model_json = model_7.to_json()\n",
        "with open('model_7.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "model_7.save_weights('model_7.h5') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFWDxcDo_i9P",
        "colab_type": "text"
      },
      "source": [
        "# Now repeating the above process for 5 Convolution layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87zwkK0YuM8i",
        "colab_type": "text"
      },
      "source": [
        "# 5 Convolution Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8fbxNGduNY4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "6d32e333-2e8a-4026-832b-806f9bc07172"
      },
      "source": [
        "from keras.layers import MaxPooling2D\n",
        "# Initialising the model\n",
        "model_5 = Sequential()\n",
        "\n",
        "# Adding first conv layer\n",
        "model_5.add(Conv2D(8, kernel_size=(5, 5),padding='same',activation='relu',input_shape=x_train.shape[1:]))\n",
        "\n",
        "# Adding second conv layer\n",
        "model_5.add(Conv2D(16, (5, 5), activation='relu'))\n",
        "\n",
        "# Adding Maxpooling layer\n",
        "model_5.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "\n",
        "\n",
        "# Adding Dropout\n",
        "model_5.add(Dropout(0.25))\n",
        "\n",
        "# Adding third conv layer\n",
        "model_5.add(Conv2D(32, (5, 5),padding='same', activation='relu'))\n",
        "\n",
        "# Adding Maxpooling layer\n",
        "model_5.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "\n",
        "# Adding Dropout\n",
        "model_5.add(Dropout(0.25))\n",
        "\n",
        "# Adding fourth conv layer\n",
        "model_5.add(Conv2D(64, (5, 5),padding='same',activation='relu'))\n",
        "\n",
        "# Adding fifth conv layer\n",
        "model_5.add(Conv2D(64, (5, 5), activation='relu'))\n",
        "\n",
        "# Adding Maxpooling layer\n",
        "model_5.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
        "\n",
        "# Adding Dropout\n",
        "model_5.add(Dropout(0.25))\n",
        "\n",
        "# Adding flatten layer\n",
        "model_5.add(Flatten())\n",
        "\n",
        "# Adding first hidden layer\n",
        "model_5.add(Dense(256, activation='relu',kernel_initializer=he_normal(seed=None)))\n",
        "\n",
        "# Adding Batch Normalization\n",
        "model_5.add(BatchNormalization())\n",
        "\n",
        "# Adding Dropout\n",
        "model_5.add(Dropout(0.5))\n",
        "\n",
        "# Adding output layer\n",
        "model_5.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Printing model Summary\n",
        "print(model_5.summary())\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_14 (Conv2D)           (None, 32, 32, 8)         608       \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 28, 28, 16)        3216      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 14, 14, 32)        12832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 7, 7, 64)          51264     \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 3, 3, 64)          102464    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 239,770\n",
            "Trainable params: 239,258\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiB9xuxouVPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4369
        },
        "outputId": "6f986010-ce5a-458b-de37-5e8f4cbe44e9"
      },
      "source": [
        "# Compiling the model\n",
        "model_5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fitting the data to the model\n",
        "history_5 = model_5.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "390/390 [==============================] - 22s 55ms/step - loss: 1.8424 - acc: 0.3008 - val_loss: 1.4924 - val_acc: 0.4461\n",
            "Epoch 2/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.5590 - acc: 0.4265 - val_loss: 1.4410 - val_acc: 0.4833\n",
            "Epoch 3/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.4484 - acc: 0.4745 - val_loss: 1.3351 - val_acc: 0.5221\n",
            "Epoch 4/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.3641 - acc: 0.5067 - val_loss: 1.4476 - val_acc: 0.5103\n",
            "Epoch 5/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.3048 - acc: 0.5346 - val_loss: 1.1611 - val_acc: 0.5961\n",
            "Epoch 6/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.2543 - acc: 0.5559 - val_loss: 1.2459 - val_acc: 0.5721\n",
            "Epoch 7/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.2189 - acc: 0.5712 - val_loss: 1.1429 - val_acc: 0.5949\n",
            "Epoch 8/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.1681 - acc: 0.5932 - val_loss: 1.0380 - val_acc: 0.6451\n",
            "Epoch 9/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.1472 - acc: 0.5961 - val_loss: 1.1215 - val_acc: 0.6182\n",
            "Epoch 10/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.1080 - acc: 0.6106 - val_loss: 0.9780 - val_acc: 0.6579\n",
            "Epoch 11/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 1.0813 - acc: 0.6244 - val_loss: 1.1067 - val_acc: 0.6282\n",
            "Epoch 12/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.0522 - acc: 0.6338 - val_loss: 1.0633 - val_acc: 0.6407\n",
            "Epoch 13/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.0320 - acc: 0.6433 - val_loss: 1.0008 - val_acc: 0.6566\n",
            "Epoch 14/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.0191 - acc: 0.6471 - val_loss: 1.0001 - val_acc: 0.6630\n",
            "Epoch 15/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 1.0019 - acc: 0.6547 - val_loss: 0.8445 - val_acc: 0.7071\n",
            "Epoch 16/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.9875 - acc: 0.6588 - val_loss: 0.9196 - val_acc: 0.6932\n",
            "Epoch 17/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.9667 - acc: 0.6654 - val_loss: 0.8912 - val_acc: 0.7007\n",
            "Epoch 18/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.9568 - acc: 0.6725 - val_loss: 0.9628 - val_acc: 0.6847\n",
            "Epoch 19/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.9459 - acc: 0.6743 - val_loss: 0.8172 - val_acc: 0.7149\n",
            "Epoch 20/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.9398 - acc: 0.6778 - val_loss: 0.8691 - val_acc: 0.7038\n",
            "Epoch 21/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.9165 - acc: 0.6831 - val_loss: 0.8320 - val_acc: 0.7155\n",
            "Epoch 22/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.9021 - acc: 0.6897 - val_loss: 0.7974 - val_acc: 0.7290\n",
            "Epoch 23/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.9028 - acc: 0.6908 - val_loss: 0.7448 - val_acc: 0.7446\n",
            "Epoch 24/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.8871 - acc: 0.6933 - val_loss: 0.8067 - val_acc: 0.7277\n",
            "Epoch 25/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.8828 - acc: 0.6975 - val_loss: 0.7519 - val_acc: 0.7418\n",
            "Epoch 26/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.8755 - acc: 0.6990 - val_loss: 0.8810 - val_acc: 0.7029\n",
            "Epoch 27/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.8611 - acc: 0.7031 - val_loss: 0.8170 - val_acc: 0.7253\n",
            "Epoch 28/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.8532 - acc: 0.7076 - val_loss: 0.7258 - val_acc: 0.7535\n",
            "Epoch 29/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8559 - acc: 0.7065 - val_loss: 0.6897 - val_acc: 0.7622\n",
            "Epoch 30/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8496 - acc: 0.7103 - val_loss: 0.7545 - val_acc: 0.7445\n",
            "Epoch 31/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8386 - acc: 0.7103 - val_loss: 0.7449 - val_acc: 0.7477\n",
            "Epoch 32/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8401 - acc: 0.7107 - val_loss: 0.6728 - val_acc: 0.7659\n",
            "Epoch 33/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.8326 - acc: 0.7140 - val_loss: 0.7824 - val_acc: 0.7351\n",
            "Epoch 34/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.8273 - acc: 0.7170 - val_loss: 0.7784 - val_acc: 0.7330\n",
            "Epoch 35/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8100 - acc: 0.7242 - val_loss: 0.7613 - val_acc: 0.7451\n",
            "Epoch 36/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8124 - acc: 0.7214 - val_loss: 0.7178 - val_acc: 0.7539\n",
            "Epoch 37/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.8088 - acc: 0.7235 - val_loss: 0.8106 - val_acc: 0.7384\n",
            "Epoch 38/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.8030 - acc: 0.7257 - val_loss: 0.6788 - val_acc: 0.7653\n",
            "Epoch 39/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.8002 - acc: 0.7258 - val_loss: 0.6641 - val_acc: 0.7749\n",
            "Epoch 40/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7885 - acc: 0.7308 - val_loss: 0.6898 - val_acc: 0.7622\n",
            "Epoch 41/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7960 - acc: 0.7276 - val_loss: 0.6970 - val_acc: 0.7615\n",
            "Epoch 42/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7863 - acc: 0.7308 - val_loss: 0.6946 - val_acc: 0.7685\n",
            "Epoch 43/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.7765 - acc: 0.7332 - val_loss: 0.7358 - val_acc: 0.7490\n",
            "Epoch 44/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7825 - acc: 0.7321 - val_loss: 0.6947 - val_acc: 0.7682\n",
            "Epoch 45/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7697 - acc: 0.7352 - val_loss: 0.7160 - val_acc: 0.7672\n",
            "Epoch 46/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.7699 - acc: 0.7348 - val_loss: 0.7150 - val_acc: 0.7619\n",
            "Epoch 47/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.7726 - acc: 0.7349 - val_loss: 0.7638 - val_acc: 0.7477\n",
            "Epoch 48/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7627 - acc: 0.7371 - val_loss: 0.7005 - val_acc: 0.7653\n",
            "Epoch 49/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7655 - acc: 0.7364 - val_loss: 0.6858 - val_acc: 0.7750\n",
            "Epoch 50/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7563 - acc: 0.7403 - val_loss: 0.6207 - val_acc: 0.7886\n",
            "Epoch 51/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.7576 - acc: 0.7404 - val_loss: 0.6775 - val_acc: 0.7680\n",
            "Epoch 52/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.7556 - acc: 0.7404 - val_loss: 0.6394 - val_acc: 0.7801\n",
            "Epoch 53/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7557 - acc: 0.7413 - val_loss: 0.6624 - val_acc: 0.7782\n",
            "Epoch 54/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.7467 - acc: 0.7435 - val_loss: 0.7567 - val_acc: 0.7530\n",
            "Epoch 55/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.7520 - acc: 0.7422 - val_loss: 0.7151 - val_acc: 0.7621\n",
            "Epoch 56/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.7411 - acc: 0.7473 - val_loss: 0.6381 - val_acc: 0.7836\n",
            "Epoch 57/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7359 - acc: 0.7485 - val_loss: 0.7458 - val_acc: 0.7538\n",
            "Epoch 58/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.7414 - acc: 0.7440 - val_loss: 0.6988 - val_acc: 0.7662\n",
            "Epoch 59/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.7338 - acc: 0.7490 - val_loss: 0.6367 - val_acc: 0.7855\n",
            "Epoch 60/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7347 - acc: 0.7484 - val_loss: 0.6626 - val_acc: 0.7786\n",
            "Epoch 61/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7234 - acc: 0.7524 - val_loss: 0.6785 - val_acc: 0.7743\n",
            "Epoch 62/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.7333 - acc: 0.7516 - val_loss: 0.6352 - val_acc: 0.7892\n",
            "Epoch 63/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7263 - acc: 0.7510 - val_loss: 0.7472 - val_acc: 0.7597\n",
            "Epoch 64/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7250 - acc: 0.7518 - val_loss: 0.7087 - val_acc: 0.7609\n",
            "Epoch 65/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7191 - acc: 0.7545 - val_loss: 0.6175 - val_acc: 0.7914\n",
            "Epoch 66/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7156 - acc: 0.7548 - val_loss: 0.6745 - val_acc: 0.7769\n",
            "Epoch 67/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.7205 - acc: 0.7538 - val_loss: 0.6474 - val_acc: 0.7802\n",
            "Epoch 68/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7081 - acc: 0.7603 - val_loss: 0.7154 - val_acc: 0.7644\n",
            "Epoch 69/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.7164 - acc: 0.7545 - val_loss: 0.6500 - val_acc: 0.7818\n",
            "Epoch 70/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7105 - acc: 0.7562 - val_loss: 0.7014 - val_acc: 0.7694\n",
            "Epoch 71/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7087 - acc: 0.7550 - val_loss: 0.6727 - val_acc: 0.7748\n",
            "Epoch 72/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.7048 - acc: 0.7592 - val_loss: 0.6241 - val_acc: 0.7910\n",
            "Epoch 73/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7026 - acc: 0.7595 - val_loss: 0.7328 - val_acc: 0.7637\n",
            "Epoch 74/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.7016 - acc: 0.7602 - val_loss: 0.6430 - val_acc: 0.7844\n",
            "Epoch 75/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.7023 - acc: 0.7622 - val_loss: 0.6325 - val_acc: 0.7869\n",
            "Epoch 76/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7068 - acc: 0.7572 - val_loss: 0.6611 - val_acc: 0.7780\n",
            "Epoch 77/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6973 - acc: 0.7627 - val_loss: 0.6652 - val_acc: 0.7784\n",
            "Epoch 78/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.7019 - acc: 0.7603 - val_loss: 0.6878 - val_acc: 0.7726\n",
            "Epoch 79/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6963 - acc: 0.7613 - val_loss: 0.6006 - val_acc: 0.7976\n",
            "Epoch 80/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6892 - acc: 0.7651 - val_loss: 0.6249 - val_acc: 0.7907\n",
            "Epoch 81/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6866 - acc: 0.7647 - val_loss: 0.6087 - val_acc: 0.7988\n",
            "Epoch 82/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6931 - acc: 0.7631 - val_loss: 0.6472 - val_acc: 0.7845\n",
            "Epoch 83/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6881 - acc: 0.7646 - val_loss: 0.6713 - val_acc: 0.7733\n",
            "Epoch 84/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.6889 - acc: 0.7631 - val_loss: 0.6386 - val_acc: 0.7840\n",
            "Epoch 85/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6852 - acc: 0.7643 - val_loss: 0.6963 - val_acc: 0.7668\n",
            "Epoch 86/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6831 - acc: 0.7655 - val_loss: 0.6165 - val_acc: 0.7925\n",
            "Epoch 87/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6794 - acc: 0.7677 - val_loss: 0.6248 - val_acc: 0.7931\n",
            "Epoch 88/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.6832 - acc: 0.7648 - val_loss: 0.6633 - val_acc: 0.7789\n",
            "Epoch 89/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6821 - acc: 0.7680 - val_loss: 0.6051 - val_acc: 0.7960\n",
            "Epoch 90/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6779 - acc: 0.7684 - val_loss: 0.6942 - val_acc: 0.7685\n",
            "Epoch 91/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6792 - acc: 0.7666 - val_loss: 0.6262 - val_acc: 0.7923\n",
            "Epoch 92/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6801 - acc: 0.7663 - val_loss: 0.5992 - val_acc: 0.7953\n",
            "Epoch 93/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6787 - acc: 0.7677 - val_loss: 0.6906 - val_acc: 0.7715\n",
            "Epoch 94/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6767 - acc: 0.7671 - val_loss: 0.6146 - val_acc: 0.8011\n",
            "Epoch 95/128\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.6709 - acc: 0.7732 - val_loss: 0.5820 - val_acc: 0.8037\n",
            "Epoch 96/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6694 - acc: 0.7699 - val_loss: 0.6102 - val_acc: 0.7950\n",
            "Epoch 97/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6724 - acc: 0.7687 - val_loss: 0.6107 - val_acc: 0.7969\n",
            "Epoch 98/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6695 - acc: 0.7706 - val_loss: 0.6135 - val_acc: 0.8002\n",
            "Epoch 99/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6761 - acc: 0.7686 - val_loss: 0.6143 - val_acc: 0.7963\n",
            "Epoch 100/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6649 - acc: 0.7739 - val_loss: 0.6327 - val_acc: 0.7878\n",
            "Epoch 101/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6693 - acc: 0.7690 - val_loss: 0.6349 - val_acc: 0.7884\n",
            "Epoch 102/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6676 - acc: 0.7707 - val_loss: 0.6867 - val_acc: 0.7795\n",
            "Epoch 103/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6652 - acc: 0.7713 - val_loss: 0.6139 - val_acc: 0.7978\n",
            "Epoch 104/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6649 - acc: 0.7722 - val_loss: 0.5884 - val_acc: 0.8039\n",
            "Epoch 105/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6607 - acc: 0.7745 - val_loss: 0.5581 - val_acc: 0.8114\n",
            "Epoch 106/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6604 - acc: 0.7731 - val_loss: 0.6875 - val_acc: 0.7770\n",
            "Epoch 107/128\n",
            "390/390 [==============================] - 21s 54ms/step - loss: 0.6628 - acc: 0.7726 - val_loss: 0.6804 - val_acc: 0.7756\n",
            "Epoch 108/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6604 - acc: 0.7729 - val_loss: 0.6634 - val_acc: 0.7798\n",
            "Epoch 109/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6555 - acc: 0.7758 - val_loss: 0.6558 - val_acc: 0.7842\n",
            "Epoch 110/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6488 - acc: 0.7763 - val_loss: 0.5963 - val_acc: 0.8010\n",
            "Epoch 111/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6549 - acc: 0.7753 - val_loss: 0.5992 - val_acc: 0.8036\n",
            "Epoch 112/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6547 - acc: 0.7745 - val_loss: 0.6409 - val_acc: 0.7884\n",
            "Epoch 113/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6574 - acc: 0.7751 - val_loss: 0.6088 - val_acc: 0.7934\n",
            "Epoch 114/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6534 - acc: 0.7762 - val_loss: 0.6618 - val_acc: 0.7844\n",
            "Epoch 115/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6493 - acc: 0.7780 - val_loss: 0.6293 - val_acc: 0.7927\n",
            "Epoch 116/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6629 - acc: 0.7718 - val_loss: 0.5987 - val_acc: 0.8003\n",
            "Epoch 117/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6505 - acc: 0.7768 - val_loss: 0.6719 - val_acc: 0.7838\n",
            "Epoch 118/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6482 - acc: 0.7776 - val_loss: 0.5922 - val_acc: 0.8004\n",
            "Epoch 119/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6445 - acc: 0.7779 - val_loss: 0.6536 - val_acc: 0.7856\n",
            "Epoch 120/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6472 - acc: 0.7765 - val_loss: 0.6160 - val_acc: 0.7952\n",
            "Epoch 121/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6448 - acc: 0.7802 - val_loss: 0.5491 - val_acc: 0.8135\n",
            "Epoch 122/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6440 - acc: 0.7813 - val_loss: 0.6685 - val_acc: 0.7844\n",
            "Epoch 123/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6503 - acc: 0.7788 - val_loss: 0.6107 - val_acc: 0.7977\n",
            "Epoch 124/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6410 - acc: 0.7823 - val_loss: 0.5825 - val_acc: 0.8061\n",
            "Epoch 125/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6458 - acc: 0.7784 - val_loss: 0.6665 - val_acc: 0.7856\n",
            "Epoch 126/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6466 - acc: 0.7788 - val_loss: 0.5708 - val_acc: 0.8080\n",
            "Epoch 127/128\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.6407 - acc: 0.7804 - val_loss: 0.6279 - val_acc: 0.7930\n",
            "Epoch 128/128\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.6390 - acc: 0.7806 - val_loss: 0.5856 - val_acc: 0.8075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fSi-JlAuNv4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "605bcfad-5d71-4fb4-828c-1e99fdb2f943"
      },
      "source": [
        "# Evaluating the model\n",
        "score = model_5.evaluate(x_test, y_test, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "fig,ax = plt.subplots(1,1)\n",
        "ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
        "\n",
        "# list of epoch numbers\n",
        "x = list(range(1,epochs+1))\n",
        "\n",
        "# print(history.history.keys())\n",
        "# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
        "# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n",
        "\n",
        "# we will get val_loss and val_acc only when you pass the paramter validation_data\n",
        "# val_loss : validation loss\n",
        "# val_acc : validation accuracy\n",
        "\n",
        "# loss : training loss\n",
        "# acc : train accuracy\n",
        "# for each key in histrory.histrory we will have a list of length equal to number of epochs\n",
        "\n",
        "vy = history_5.history['val_loss']\n",
        "ty = history_5.history['loss']\n",
        "plt_dynamic(x, vy, ty, ax)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.5856034601449966\n",
            "Test accuracy: 0.8075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiI6UhQiIqIjRSEiIjr+CybzS3nMZ0BMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEr4uweuulJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save to disk\n",
        "model_json1 = model_5.to_json()\n",
        "with open('model_5.json', 'w') as json_file:\n",
        "    json_file.write(model_json1)\n",
        "model_5.save_weights('model_5.h5') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGiapjI_ri_",
        "colab_type": "text"
      },
      "source": [
        "# Result:\n",
        "**We see that model with 7 convolution layer wis working good**"
      ]
    }
  ]
}